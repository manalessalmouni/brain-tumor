# ğŸ§  Brain Tumor Classification using Deep Learning (CNN)

## ğŸ“Œ Project Overview

This project focuses on the **automatic classification of brain tumors from MRI images** using **Deep Learning** techniques.  
A **Convolutional Neural Network (CNN) built from scratch** is designed and trained to classify MRI scans into **four categories**:

- **Glioma**
- **Meningioma**
- **Pituitary Tumor**
- **No Tumor**

The project demonstrates the effectiveness of CNNs in **medical image analysis** and their potential as **decision-support tools** for radiologists.

---

## ğŸ¯ Objectives

- Build a **CNN from scratch** for medical image classification  
- Apply **data preprocessing and augmentation** techniques  
- Achieve high classification accuracy on MRI images  
- Analyze model performance using quantitative and visual metrics  

---

## ğŸ“‚ Dataset

The dataset used is the **Brain Tumor MRI Dataset**, organized as follows:

Dataset/
â”‚
â”œâ”€â”€ Training/
â”‚ â”œâ”€â”€ glioma/
â”‚ â”œâ”€â”€ meningioma/
â”‚ â”œâ”€â”€ pituitary/
â”‚ â””â”€â”€ notumor/
â”‚
â””â”€â”€ Testing/
â”œâ”€â”€ glioma/
â”œâ”€â”€ meningioma/
â”œâ”€â”€ pituitary/
â””â”€â”€ notumor/

- Images are grayscale or RGB MRI scans
- Each folder name represents the corresponding class
- The dataset is relatively balanced across classes

---

## ğŸ–¼ï¸ Data Preprocessing

- Image resizing to **150 Ã— 150 pixels**
- Pixel normalization to the range **[0, 1]**
- One-hot encoding of class labels
- Train/validation/test split

---

## ğŸ”„ Data Augmentation

- Random rotations  
- Width and height shifts  
- Shear transformations  
- Zoom operations  
- Horizontal flipping  

---

## ğŸ§  Model Architecture

- **4 Convolutional layers**: Filters 32 â†’ 64 â†’ 128 â†’ 128, kernel size 4Ã—4, ReLU activation  
- **MaxPooling layers** after each convolution (3Ã—3)  
- **Fully connected layers**: Flatten â†’ Dense(512, ReLU) â†’ Dropout(0.5)  
- **Output layer**: Dense(4, Softmax)  

---

## ğŸ“‰ Loss Function and Optimizer

- **Loss Function:** Categorical Crossentropy  
- **Optimizer:** Adam (learning rate 0.001)  
- **Evaluation Metric:** Accuracy  

---

## â±ï¸ Training Strategy

- Batch size: **32**
- Maximum epochs: **40**
- Callbacks:
  - **EarlyStopping** to prevent overfitting
  - **ReduceLROnPlateau** to adjust learning rate dynamically

---

## ğŸ“Š Results

- **Test Accuracy:** ~95%  
- High precision, recall, and F1-score across all classes  
- Confusion matrix shows most misclassifications occur between **glioma and meningioma**, which is clinically reasonable.

---

## ğŸ“ˆ Evaluation and Visualization

- Training and validation accuracy/loss curves  
- Confusion matrix  
- Sample predictions with true vs predicted labels  

---

## ğŸ› ï¸ Technologies Used

- Python  
- TensorFlow / Keras  
- NumPy  
- Matplotlib  
- Scikit-learn  

---

## ğŸš€ Future Improvements

- Apply **Transfer Learning** (VGG16, ResNet50, EfficientNet)  
- Add **model interpretability** using Grad-CAM  
- Train on larger and real clinical datasets  
- Deploy as a **web or desktop application**  

---

## âš ï¸ Disclaimer

This project is intended **for educational and research purposes only**.  
It is **not a medical diagnostic tool** and should not be used as a substitute for professional medical advice.

---
ğŸ§  DenseNet121 â€“ Classification des Images MÃ©dicales (IRM / Radiographies)
ğŸ“Œ Description

Cette partie du projet implÃ©mente un modÃ¨le DenseNet121 basÃ© sur le Transfer Learning pour la classification multi-classes dâ€™images mÃ©dicales.
Le modÃ¨le est entraÃ®nÃ© pour distinguer entre plusieurs catÃ©gories cliniques (par exemple : glioma, meningioma, notumor, pituitary), Ã  partir dâ€™images IRM / radiographiques.

DenseNet121 est particuliÃ¨rement adaptÃ© aux applications mÃ©dicales grÃ¢ce Ã  :

une meilleure propagation des gradients,

une rÃ©utilisation efficace des caractÃ©ristiques,

une rÃ©duction du sur-apprentissage sur des datasets de taille limitÃ©e.
âš™ï¸ PrÃ©traitement des DonnÃ©es

Les Ã©tapes de prÃ©traitement appliquÃ©es sont :

Redimensionnement des images Ã  224 Ã— 224

Normalisation des pixels

Conversion en RGB (3 canaux)

Augmentation de donnÃ©es (training uniquement) :

Rotation (Â±15Â°)

Translation (Â±10%)

Zoom (Â±10%)

Flip horizontal

Ces techniques amÃ©liorent la robustesse et la capacitÃ© de gÃ©nÃ©ralisation du modÃ¨le.

ğŸ—ï¸ Architecture du ModÃ¨le

Le modÃ¨le DenseNet121 est utilisÃ© comme extracteur de caractÃ©ristiques, avec des poids prÃ©-entraÃ®nÃ©s sur ImageNet.

ğŸ”¹ Pipeline du modÃ¨le :

DenseNet121 (Base gelÃ©e)

Global Average Pooling

Dense (512) + ReLU

Batch Normalization + Dropout

Dense (256) + ReLU

Batch Normalization + Dropout

Dense (128) + ReLU

Dense (N_classes) + Softmax

Cette architecture permet un bon compromis entre performance et complexitÃ©.

ğŸ§ª EntraÃ®nement

Fonction de perte : Categorical Crossentropy

Optimiseur : Adam

Batch size : 32

Nombre dâ€™Ã©poques : 20 (+ fine-tuning optionnel)

StratÃ©gie : Transfer Learning + Fine-tuning partiel

ğŸ“Š RÃ©sultats

Les performances du modÃ¨le sont Ã©valuÃ©es Ã  lâ€™aide de :

Courbes Accuracy / Loss (Train & Validation)

Matrice de confusion

Precision, Recall, F1-score par classe

DenseNet121 montre une excellente capacitÃ© de classification, en particulier pour les classes cliniquement distinctes, avec une bonne stabilitÃ© entre entraÃ®nement et validation.

ğŸ“ Fichiers Importants

densenet_train.ipynb : entraÃ®nement du modÃ¨le

densenet_evaluation.ipynb : Ã©valuation et mÃ©triques

confusion_matrix.png : matrice de confusion

accuracy_loss.png : courbes dâ€™apprentissage

model_densenet121.h5 : modÃ¨le entraÃ®nÃ©

ğŸš€ ExÃ©cution

Monter Google Drive

VÃ©rifier la structure du dataset

Lancer le notebook dâ€™entraÃ®nement

Ã‰valuer le modÃ¨le sur le jeu de test

ğŸ“š RÃ©fÃ©rences

Huang et al., Densely Connected Convolutional Networks, CVPR 2017

ImageNet Dataset

TensorFlow & Keras Documentation
## ğŸ‘©â€ğŸ“ Author

Masterâ€™s Degree â€“ Artificial Intelligence  
 Deep Learning Project
